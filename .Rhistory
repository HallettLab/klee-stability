## define a function to quantify the percentile that a given precipitation record falls into from 1-100
ppt_per_func <- function (precip, q, ...) {
for (i in 1:length(q$percentile)) {
if (precip <= q$ppt_value[i]) {
if (i == 1) {
percentile <- i
}else{
percentile <- i - 1
}
break()
}
}
return(percentile)
}
## testing the function
ppt_per_func(precip = 494.4, q = quant)
## make a vector of the number of years to iterate over
nyears <- length(drought_record$preceding12ppt)
## make an empty dataframe to contain the output
dperc_record <- data.frame(preceding12ppt = NA, sample_date = NA, drought = NA, percentile = NA)
## Define drought severity:
## severity = 1-((precip_percentile - 1)/25)
## calculate the percentiles from 0-100 of the preceipitation record
quant <- data.frame(percentile = c(0:100), ppt_value = quantile(drought_record$preceding12ppt, seq(0, 1, .01))) %>%
arrange(percentile)
## since this includes 0th quantile, it won't fit well into the defined severity function
## define a function to quantify the percentile that a given precipitation record falls into from 1-100
ppt_per_func <- function (precip, q, ...) {
for (i in 1:length(q$percentile)) {
if (precip <= q$ppt_value[i]) {
if (i == 1) {
percentile <- i
}else{
percentile <- i - 1
}
break()
}
}
return(percentile)
}
## testing the function
ppt_per_func(precip = 494.4, q = quant)
## Use the precipitation function for every ppt value in the ppt record
## make a vector of the number of years to iterate over
nyears <- length(drought_record$preceding12ppt)
## make an empty dataframe to contain the output
dperc_record <- data.frame(preceding12ppt = NA, sample_date = NA, drought = NA, percentile = NA)
## iterate over every year
for (j in 1:nyears) {
## select the particular ppt record
ppt <- drought_record$preceding12ppt[j]
## filter out the correct row in drought_record
dtemp <- drought_record %>%
filter(preceding12ppt %in% ppt)
## use the function defined above to calculate quantile
percentile <- ppt_per_func(precip = ppt, q = quant)
## append this to the temp data frame in new column
dtemp$percentile <- percentile
## append temp dataframe to output dataframe
dperc_record <- rbind(dperc_record, dtemp)
}
dperc_record <- dperc_record[-1,]
## Calculate drought severity:
d_severity <- dperc_record %>%
mutate(severity = ifelse(drought == "1", (1 - ((percentile - 1)/25)), 0))
## first, need to define whether there is a drought in the previous year
d_sever_prev <- d_severity %>%
mutate(drought_prev = ifelse(drought == 1, 0,
ifelse(lag(drought, default = 0) == 1, 1, 0)))
### CREATE Drought Scoring FUNCTION ###
dr_score_func <- function(input_data, timestep, ...) {
## function inputs = data frame and number of time steps
## input_data should be d_sever_prev
## timestep is the window_size
## number of sampling points
n_samples <- length(unique(input_data$sample_date))
## number of windows to iterate over
n_windows <- n_samples - timestep + 1
## create ordered list of sample points
sample_points <- sort(unique(input_data$sample_date))
## create dataframe to contain output
Dscore <- as.data.frame(matrix(nrow = n_windows, ncol = 4))
## rename columns
colnames(Dscore) <- c("timestep", "Dscore", "num_drought", "contains_drought")
## set default values of drought_flags
Dscore$timestep <- c(1:n_windows)
Dscore$contains_drought <- 0
Dscore$Dscore <- 0
Dscore$num_drought <- 0
## iterate over every window in the time series
for (i in 1:n_windows){
## create a vector of sample points for each iteration
temp_samplepts <- sample_points[i:(i+timestep-1)]
## filter the correct sample points from input data
temp <- input_data %>%
filter(sample_date %in% temp_samplepts)
## create a vector of index positions
index_position  <- unique(temp$sample_date)
## create empty dataframe to contain output of second loop
droughtIDs <- data.frame(sample_date = NA, Dscore_indiv = NA, drought = NA, timestep = NA)
## iterate through each YEAR in the window
for (j in 1:length(index_position)){
## need to separate each date first
date <- index_position[j]
## identify & score individual droughts in a window
tempDrID <- temp %>%
filter(sample_date %in% date) %>%
mutate(Dscore_indiv = ifelse(drought == "1", 1*severity,
ifelse(drought_prev == "1",
0.5*severity, 0))) %>%
select(-preceding12ppt, -percentile, -severity, -drought_prev)
## make a column for timestep
tempDrID$timestep <- i
## combine with the empty dataframe to save outputs
droughtIDs <- rbind(droughtIDs, tempDrID) %>%
filter(!is.na(sample_date)) ## empty dataframe contains a row of NAs, remove this
}
## sum the individual drought score of each year in the window so that every window has a final Drought Score
meantempDrID <- droughtIDs %>%
group_by(timestep) %>%
summarise(Dscore = (sum(Dscore_indiv)/length(index_position)), num_drought = sum(drought)) %>%
mutate(contains_drought = ifelse(Dscore > 0, 1, 0))
## if there is a drought in a window, attach that row into the empty drought_relevance dataframe
if(meantempDrID$contains_drought == "1"){
Dscore[i,] <- meantempDrID
}
}
return(Dscore) ## retrieve data frame at the end
}
dscore10 <- dr_score_func(input_data = d_sever_prev, timestep = 10)
dscore5 <- dr_score_func(input_data = d_sever_prev, timestep = 5)
## select stability 5 & 10 year windows
stabmw5 <- stab_mw_tx %>%
filter(window_size == "5")
stabmw10 <- stab_mw_tx %>%
filter(window_size == "10")
## select classic VR 5 & 10 year windows
cvrmw5 <- cVR_mw_tx %>%
filter(window_size == "5")
cvrmw10 <- cVR_mw_tx %>%
filter(window_size == "10")
## select population stability 5 & 10 year windows
spstmw10 <- popst_mw_tx %>%
filter(window_size == "10")
spstmw5 <- popst_mw_tx %>%
filter(window_size == "5")
## select richness 5 & 10 year windows
richmw5 <- rich_mw_tx %>%
filter(window_size == "5")
richmw10 <- rich_mw_tx %>%
filter(window_size == "10")
## Does the classic VR correlate with Stability changes over time?
stab_cvr_mw <- left_join(stab_mw_tx, cVR_mw_tx, by = c("TREATMENT", "timestep", "window_size")) %>%
filter(window_size > 4, window_size < 16)
## create vector of treatments
trt <- unique(stab_cvr_mw$TREATMENT)
## create vector of window size
wsize <- unique(stab_cvr_mw$window_size)
## create empty dataframe to contain output
corr_cvr <- data.frame(TREATMENT = NA, window_size = NA, corr_coefficient = NA, p_val = NA)
for (i in 1:length(trt)) {
## select the treatment
tx <- trt[i]
## filter by treatment
temptx <- stab_cvr_mw %>%
filter(TREATMENT %in% tx)
for (j in 1:length(wsize)){
## select window size
wind <- wsize[j]
## filter by window size
tempw <- temptx %>%
filter(window_size %in% wind)
## feed through cor.test function
cortst <- cor.test(tempw$mean_cVR, tempw$mean_stability,
alternative = "two.sided",
method = "pearson")
## create a dataframe with the correlation coefficient and p-value
t <- tempw %>%
group_by(TREATMENT) %>%
summarise(window_size = mean(window_size)) %>%
mutate(corr_coefficient = cortst$estimate, p_val = cortst$p.value)
## bind to output dataframe
corr_cvr <- rbind(corr_cvr, t)
}
}
corr_cvr <- corr_cvr[-1,]
## create a variable indicating significance of p-value
corr_cvr <- corr_cvr %>%
mutate(significant = ifelse(p_val < 0.051, "S",
ifelse(p_val > 0.05 & p_val < 0.07, "M", "NS")))
print(corr_cvr)
## Stability Richness Correlation test
##create data frame for correlation test
stab_rich_mw <- left_join(stab_mw_tx, rich_mw_tx, by = c("TREATMENT", "timestep", "window_size")) %>%
filter(window_size > 3, window_size < 16)
## create vector of treatments
trt <- unique(stab_rich_mw$TREATMENT)
## create vector of window size
wsize <- unique(stab_rich_mw$window_size)
corr_rich <- data.frame(TREATMENT = NA, window_size = NA, corr_coefficient = NA, p_val = NA) ## create empty dataframe to contain output
for (i in 1:length(trt)) {
## select the ith treatment
tx <- trt[i]
## filter by treatment
temptx <- stab_rich_mw %>%
filter(TREATMENT %in% tx)
for (j in 1:length(wsize)){
## select window size
wind <- wsize[j]
## filter by window size
tempw <- temptx %>%
filter(window_size %in% wind)
## feed through cor.test function
cortst <- cor.test(tempw$mean_rich, tempw$mean_stability,
alternative = "two.sided",
method = "pearson")
## create a dataframe with the correlation coefficient and p-value
t <- tempw %>%
group_by(TREATMENT) %>%
summarise(window_size = mean(window_size)) %>%
mutate(corr_coefficient = cortst$estimate, p_val = cortst$p.value)
## bind to output dataframe
corr_rich <- rbind(corr_rich, t)
}
}
corr_rich <- corr_rich[-1,]
## create a variable indicating significance of p-value
corr_rich <- corr_rich %>%
mutate(significant = ifelse(p_val < 0.051, "S",
ifelse(p_val > 0.05 & p_val < 0.07, "M", "NS")))
print(corr_rich)
## Stability Richness Correlation test
##create data frame for correlation test
stab_rich_mw <- left_join(stab_mw_tx, rich_mw_tx, by = c("TREATMENT", "timestep", "window_size")) %>%
filter(window_size > 3, window_size < 16)
## create vector of treatments
trt <- unique(stab_rich_mw$TREATMENT)
## create vector of window size
wsize <- unique(stab_rich_mw$window_size)
corr_rich <- data.frame(TREATMENT = NA, window_size = NA, corr_coefficient = NA, p_val = NA) ## create empty dataframe to contain output
## loop the correlation test over every treatment and every window size of calculation
for (i in 1:length(trt)) {
## select the ith treatment
tx <- trt[i]
## filter by treatment
temptx <- stab_rich_mw %>%
filter(TREATMENT %in% tx)
for (j in 1:length(wsize)){
## select window size
wind <- wsize[j]
## filter by window size
tempw <- temptx %>%
filter(window_size %in% wind)
## feed through cor.test function
cortst <- cor.test(tempw$mean_rich, tempw$mean_stability,
alternative = "two.sided",
method = "pearson")
## create a dataframe with the correlation coefficient and p-value
t <- tempw %>%
group_by(TREATMENT) %>%
summarise(window_size = mean(window_size)) %>%
mutate(corr_coefficient = cortst$estimate, p_val = cortst$p.value)
## bind to output dataframe
corr_rich <- rbind(corr_rich, t)
}
}
corr_rich <- corr_rich[-1,]
## create a variable indicating significance of p-value
corr_rich <- corr_rich %>%
mutate(significant = ifelse(p_val < 0.051, "S",
ifelse(p_val > 0.05 & p_val < 0.07, "M", "NS")))
print(corr_rich)
# Stability - Pop Stability Correlation test
##create data frame for correlation test
stab_popst_mw <- left_join(stab_mw_tx, popst_mw_tx, by = c("TREATMENT", "timestep", "window_size")) %>%
filter(window_size > 3, window_size < 16)
## create vector of treatments
trt <- unique(stab_popst_mw$TREATMENT)
## create vector of window size
wsize <- unique(stab_popst_mw$window_size)
corr_popst <- data.frame(TREATMENT = NA, window_size = NA, corr_coefficient = NA, p_val = NA) ## create empty dataframe to contain output
## loop the correlation test over every treatment and every window size of calculation
for (i in 1:length(trt)) {
## select the ith treatment
tx <- trt[i]
## filter by treatment
temptx <- stab_popst_mw %>%
filter(TREATMENT %in% tx)
for (j in 1:length(wsize)){
## select window size
wind <- wsize[j]
## filter by window size
tempw <- temptx %>%
filter(window_size %in% wind)
## feed through cor.test function
cortst <- cor.test(tempw$avgpopstab, tempw$mean_stability,
alternative = "two.sided",
method = "pearson")
## create a dataframe with the correlation coefficient and p-value
t <- tempw %>%
group_by(TREATMENT) %>%
summarise(window_size = mean(window_size)) %>%
mutate(corr_coefficient = cortst$estimate, p_val = cortst$p.value)
## bind to output dataframe
corr_popst <- rbind(corr_popst, t)
}
}
corr_popst <- corr_popst[-1,]
## create a variable indicating significance of p-value
corr_popst <- corr_popst %>%
mutate(significant = ifelse(p_val < 0.051, "S",
ifelse(p_val > 0.05 & p_val < 0.07, "M", "NS")))
print(corr_popst)
setwd("~/Repositories/klee-stability")
## Load packages
library(tidyverse)
library(vegan)
library(forcats)
library(lubridate)
library(codyn)
library(ggpubr)
library(lme4)
library(nlme)
library(emmeans)
library(car)
library(MuMIn)
library(scico)
library(tsvr)
library(knitr)
library(data.table)
library(segmented)
library(breakpoint)
library(ggrepel)
library(lme4)
## source cleaned data
source("klee_allyears_cleaning.R")
ppt <- read.csv("drought_record.csv")
klee_annual <- klee_annual %>%
mutate(TREATMENT = fct_relevel(TREATMENT, "O", "W", "MW", "C", "WC", "MWC")) #reorder treatments
klee_annual$Date_numeric <- as.numeric(klee_annual$Date_numeric)
## Calculate Community Stability for all years
commstab <- community_stability(klee_annual,
time.var = "Date_numeric",
abundance.var = "Pin_Hits",
replicate.var = "Unique_ID")
## Calculate Variance of the full community
v <- klee_annual %>%
group_by(TREATMENT, Unique_ID) %>%
summarise(variance = var(Pin_Hits))
## Calculate Total Cover
pinhits <- totcov %>% #this dataframe used in models
group_by(TREATMENT, Unique_ID) %>%
summarize(pinhits = mean(totcov))
## join stability measures with treatment info & calculate CV
tstab <- left_join(commstab, treats, by = "Unique_ID") %>% #join comm stability with treatment info
mutate(CV = 1/stability) #calculate the CV as inverse of stability
variance <- left_join(v, pinhits, by = c("TREATMENT", "Unique_ID"))
stability <- left_join(tstab, variance, by = c("TREATMENT", "Unique_ID"))
rm(list = c("v", "commstab", "variance", "tstab"))
## Calculate Timescale Specific VR ##
#set up data frame to put tsvr into
outnames <- c("Unique_ID", "TREATMENT", "classicVR", "longVR", "shortVR") #column names
siteout <- as.data.frame(matrix(nrow=0, ncol = 5)) #make empty dataframe
names(siteout) <- outnames #set names for empty dataframe
plots <- unique(klee_annual$Unique_ID) #make vector of unique plots
## Use for loop to calculate TSVR for each plot
for (i in 1:length(plots)) {
#subset by replicate (gives all observations from one plot over time)
plot <- subset(klee_annual, Unique_ID == plots[i]) %>%
tbl_df()
#select species and fill 0's
plot2 <- plot %>%
select(Date_numeric, SPECIES, Pin_Hits) %>% #selecting only these three columns
spread(SPECIES, Pin_Hits, fill = 0) #changing to wide format
#transpose the data
dat <- t(as.matrix(plot2[,2:dim(plot2)[2]]))
#create a dataframe with replicate info to attach VR metrics to
VR_plots <- plot %>%
select(Unique_ID, TREATMENT, BLOCK) %>%
unique()
#calculate classic VR
res0 <- vreq_classic(dat)
VR_plots$classicVR <- res0[[3]] #extracting classic VR
#calculate tsvr
res <- tsvreq_classic(dat)
#aggregate short vs. long variance ratios
resLong <- aggts(res, res$ts[res$ts>=4]) #grabbing tsvr with time period >= 4 years
resShort <- aggts(res, res$ts[res$ts<4]) #grabbing tsvr with time period <4 years
#attach short & long variance ratios
VR_plots$longVR <- resLong[[3]]
VR_plots$shortVR <- resShort[[3]]
#append to external dataframe
siteout<-rbind(siteout, VR_plots)
}
#rename data frame
tsVR <- siteout #%>%
# mutate(community_type = "All_Species")
## Calculate the mean and standard error of VR metrics
meantsVR <- tsVR %>%
pivot_longer(cols = classicVR:shortVR, names_to = "VR_type", values_to = "VR_value" ) %>% #change to long format
group_by(TREATMENT, VR_type) %>%
summarise(meanVR = mean(VR_value), SEVR = calcSE(VR_value))
#reorder treatments to match grazing pressure.
meantsVR$TREATMENT <- as.factor(meantsVR$TREATMENT) #change treatment to factor
meantsVR$TREATMENT <- factor(meantsVR$TREATMENT, levels = c("O", "W",  "MW",  "C", "WC", "MWC"))
## Calculate Timescale Specific VR ##
#set up data frame to put tsvr into
b5outnames <- c("Unique_ID", "TREATMENT", "classicVR", "longVR", "shortVR") #column names
b5siteout <- as.data.frame(matrix(nrow=0, ncol = 5)) #make empty dataframe
names(b5siteout) <- b5outnames #set names for empty dataframe
b5plots <- unique(big5annual$Unique_ID) #make vector of unique plots
## Use for loop to calculate TSVR for each plot
for (i in 1:length(b5plots)) {
#subset by replicate (gives all observations from one plot over time)
plot <- subset(big5annual, Unique_ID == b5plots[i]) %>%
tbl_df()
#select species and fill 0's
plot2 <- plot %>%
select(Date_numeric, SPECIES, Pin_Hits) %>% #selecting only these three columns
spread(SPECIES, Pin_Hits, fill = 0) #changing to wide format
#transpose the data
dat <- t(as.matrix(plot2[,2:dim(plot2)[2]]))
#create a dataframe with replicate info to attach VR metrics to
VR_plots <- plot %>%
select(Unique_ID, TREATMENT, BLOCK) %>%
unique()
#calculate classic VR
res0 <- vreq_classic(dat)
VR_plots$b5classicVR <- res0[[3]] #extracting classic VR
#calculate tsvr
res <- tsvreq_classic(dat)
#aggregate short vs. long variance ratios
resLong <- aggts(res, res$ts[res$ts>=4]) #grabbing tsvr with time period >= 4 years
resShort <- aggts(res, res$ts[res$ts<4]) #grabbing tsvr with time period <4 years
#attach short & long variance ratios
VR_plots$b5longVR <- resLong[[3]]
VR_plots$b5shortVR <- resShort[[3]]
#append to external dataframe
b5siteout<-rbind(b5siteout, VR_plots)
}
#rename data frame
b5tsVR <- b5siteout #%>%
# mutate(community_type = "Dominant")
## Calculate the mean and standard error of VR metrics
b5meantsVR <- b5tsVR %>%
pivot_longer(cols = b5classicVR:b5shortVR, names_to = "VR_type", values_to = "VR_value" ) %>% #change to long format
group_by(TREATMENT, VR_type) %>%
summarise(meanVR = mean(VR_value), SEVR = calcSE(VR_value))
#reorder treatments to match grazing pressure.
b5meantsVR$TREATMENT <- as.factor(b5meantsVR$TREATMENT) #change treatment to factor
b5meantsVR$TREATMENT <- factor(b5meantsVR$TREATMENT, levels = c("O", "W",  "MW",  "C", "WC", "MWC"))
## Full Time Series
## calculate Berger-Parker Dominance for each unique ID at each date
BP_dominance <- klee_annual %>%
group_by(TREATMENT, Unique_ID, Date_final) %>% #group by treatment, plot, and date
mutate(rank = rank(Pin_Hits, na.last = NA, ties.method = "average")) %>% #rank species in order of abundance
mutate(tot_abund = sum(Pin_Hits)) %>% #create column for total abundance in each plot
filter(rank == max(rank)) %>% #only include most abundant species in each plot
summarise(BP_dominance = Pin_Hits/tot_abund) %>% #calculate Berger-Parker dominance index
mutate(date = "all")
## Calculate the mean Berger-Parker dominance by:
## TREATMENT and UNIQUE ID - to use eventually for linear models
meanplot_BPdom <- BP_dominance %>%
group_by(TREATMENT, Unique_ID) %>%
summarize(mean_dom = mean(BP_dominance), SEdom = calcSE(BP_dominance), vardom = var(BP_dominance),
CVdom = mean_dom/sd(BP_dominance))
dominance <- BP_dominance %>%
group_by(TREATMENT, Date_final) %>%
summarize(mean_dom = mean(BP_dominance), SE_dom = calcSE(BP_dominance))
## Calculate population stability of individual dominant species
dompopstab <- big5annual %>%
group_by(TREATMENT, Unique_ID, SPECIES) %>%
summarise(temp_mean = mean(Pin_Hits), sdhits = sd(Pin_Hits), sp_stability = temp_mean/sdhits)
meandompopstab <- dompopstab %>%
#ungroup() %>%
group_by(TREATMENT, SPECIES) %>%
summarise(popstability = mean(sp_stability), SEpop = calcSE(sp_stability)) #%>%
dompopstab$TREATMENT <- as.factor(dompopstab$TREATMENT)
dompopstab <- dompopstab %>%
mutate(TREATMENT = fct_relevel(TREATMENT, "O", "W", "MW", "C", "WC", "MWC"))
## Calculate aggregate stability of dominant 5 species
## Calculate Community Stability for all years
domsp_agg_stab <- community_stability(big5annual,
time.var = "Date_numeric",
abundance.var = "Pin_Hits",
replicate.var = "Unique_ID")
colnames(domsp_agg_stab) <- c("Unique_ID", "Domsp_stability")
## I don't think this is the right way to do this... this is then just looking at does the stability of a subset of the community correlate with the stability of the full community - which is highly likely especially if picking the most abundant species...
## Could try the average population stability of dominant species or pick the most dominant species across the time series
## Average population stability of dominant species
meanpopstab <- dompopstab %>%
group_by(TREATMENT, Unique_ID) %>%
summarize(meanpopstab = mean(sp_stability))
rnorm(1000)
rexp(1000)
runif(1000)
rgamma(1000)
?rgamma
rgamma(1000, shape = a)
rgamma(1000, shape = 1)
rgamma(1000, shape = 2)
rpois(1000)
rgeom(1000)
rpois(1000, lambda = 1)
rgeom(1000, prob = 1)
rgeom(1000, prob = 2)
rgeom(1000, prob = 1)
rbinom(1000)
rbinom(1000, size = 1)
rbinom(1000, size = 1, prob = 1)
ifelse(runif(1)) < 0.5, "H", "T"
ifelse(runif(1) < 0.5, "H", "T")
